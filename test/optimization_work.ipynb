{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## An optimization test bed.\n",
    "\n",
    "First, I'll define the usual functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(X):\n",
    "    '''Check whether a matrix X is pos definite.\n",
    "    Returns True or False, depending on X.\n",
    "    '''\n",
    "    return np.all(np.linalg.eigvals(X) > 0)\n",
    "\n",
    "def batch_data(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)\n",
    "\n",
    "def sghmc(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import some optimization things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just look at optimization for the mixture of normals problem. I don't care about performance of the sampler itself right now, so I turn down the size of n and niter just to make it go relatively quickly in my tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 100 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 50\n",
    "batch_size=20 # make this smallish\n",
    "\n",
    "# Don't actually run sampling algorithm here\n",
    "# samps = sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cleaning up the multivariate normal sampling. Pre-calculate the Cholesky decompositions, and use the Cholesky-based sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just try throwing `@jit` in front of things and seeing how much improvement we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_data_numba(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data_numba(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the batch_data_numba version won't actually help things... It may just add overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba2(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size) # use original batch_data\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the times for the above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7 s ± 20.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56 s ± 25.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 s ± 33.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data_numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 s ± 13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step should be to do something more fancy, like maybe convert things to C++/Cython. You can't really parallelize the outer epoch loop or the batch function loop, because there is dependency in the Markov chain.\n",
    "\n",
    "Below, I write the C++ code and test it using Fig 1 (the toy example from the paper). Note that the way I have this written, the gradient function has to also be written in C++, which is a pain because we can't use the auto-gradient like we have in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE: Unused things that I didn't end up keeping but want to save just in case are in this block.\n",
    "\n",
    "// checks whether a matrix X is pos definite, returns bool type\n",
    "bool not_is_pos_def(Eigen::MatrixXd xs){\n",
    "    bool res = false; // assume xs is not pos definite\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfX(xs); // compute the Cholesky decomposition of xs\n",
    "    if(lltOfX.info() != Eigen::NumericalIssue){res = true;} // check if we did not get error  \n",
    "    return res;\n",
    "}\n",
    "\n",
    "// get gradient for mixture of normals example\n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    int p = theta.rows();\n",
    "    Eigen::MatrixXd star = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd star_prime0 = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd star_prime1 = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<n; i++) {\n",
    "        star(i,0) = 0.5*(-0.5*(theta(0,0)-x(i,0)).pow(2)).exp() + 0.5*(-0.5*(theta(1,0)-x(i,0)).pow(2)).exp();\n",
    "        star_prime0(i,0) = 0.5*(-0.5*(theta(0,0)-x(i,0)).pow(2)).exp()*(theta(0,0)-x(i,0));\n",
    "        star_prime1(i,0) = 0.5*(-0.5*(theta(1,0)-x(i,0)).pow(2)).exp()*(theta(1,0)-x(i,0));\n",
    "    }\n",
    "    grad(0,0) = -theta(0,0)/10 - (n/batch_size)*(star_prime0.array()/star.array()).sum();\n",
    "    grad(1,0) = -theta(1,0)/10 - (n/batch_size)*(star_prime1.array()/star.array()).sum();\n",
    "    return grad;\n",
    "}\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrap.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file wrap.cpp\n",
    "<%\n",
    "cfg['compiler_args'] = ['-std=c++11']\n",
    "cfg['include_dirs'] = ['../notebooks/eigen3']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <stdexcept>\n",
    "#include <algorithm> // std::random_shuffle\n",
    "#include <random>\n",
    "\n",
    "#include <Eigen/LU>\n",
    "#include <Eigen/Dense>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "        \n",
    "// start random number engine with fixed seed\n",
    "default_random_engine re{1234};\n",
    "// set up random normal rnorm to work like in python\n",
    "normal_distribution<double> norm(0, 1); // mean and standard deviation\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "// fill xs with draws from N(0,1) and return this n x 1 dim vector\n",
    "Eigen::MatrixXd rnorm_vec(int n) {\n",
    "    Eigen::MatrixXd xs = Eigen::MatrixXd::Zero(n, 1);\n",
    "    for (int i=0; i<n; i++) {xs(i,0) = rnorm();}\n",
    "    return xs;\n",
    "}\n",
    "    \n",
    "// get noisy gradient of Fig1 example from Chen et. al. paper\n",
    "Eigen::MatrixXd gradU_noisyFig1(Eigen::MatrixXd theta) {\n",
    "    Eigen::MatrixXd xs = -4*theta.array() + 4*theta.array().pow(3) + 2*rnorm_vec(theta.rows()).array();\n",
    "    return xs;\n",
    "} \n",
    "    \n",
    "// Broken gradient calculation\n",
    "// get gradient for mixture of normals example\n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    int p = theta.rows();\n",
    "    Eigen::ArrayXd c_0 = theta(0,0) - x.array();\n",
    "    Eigen::ArrayXd c_1 = theta(1,0) - x.array();\n",
    "    Eigen::ArrayXd star = 0.5 * (-0.5 * c_0.pow(2)).exp() + 0.5 * (-0.5 * c_1.pow(2)).exp();\n",
    "    Eigen::ArrayXd star_prime;\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<p; i++) {\n",
    "        star_prime = 0.5 * (-0.5 * (theta(i,0) - x.array()).pow(2)).exp() * (theta(i,0) - x.array());\n",
    "        grad(i,0) = -theta(i,0)/10 - (n/batch_size)*(star_prime/star).sum();\n",
    "    }\n",
    "    return grad;\n",
    "} \n",
    "    \n",
    "/*\n",
    "'''\n",
    "def gradU_test(theta, x, n, batch_size):\n",
    "    grad = np.zeros((2,1))\n",
    "    for p in range(2):\n",
    "        star = 0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)\n",
    "        star_prime = 0.5 * np.exp(-0.5*(theta[p]-x)**2) * (theta[p]-x)\n",
    "        grad[p,0] = -theta[p]/10 - (n/batch_size)*np.sum(star_prime/star)\n",
    "    return grad\n",
    "    \n",
    "// get correlation between two VectorXd type objects of same length\n",
    "double corr(Ref<VectorXd const> const& x, Ref<VectorXd const> const& y) {\n",
    "    ArrayXd c_x = x.array() - x.mean(); // centered x\n",
    "    ArrayXd c_y = y.array() - y.mean(); // centered y   \n",
    "    return (c_x * c_y).sum() / sqrt(((c_x*c_x).sum())*((c_y*c_y).sum()));\n",
    "}\n",
    "'''\n",
    "*/\n",
    "\n",
    "/* \n",
    "SGHMC algorithm as described in the paper by Chen et al.\n",
    "Note that users specify the choice of which gradient function to use by gradU_choice, \n",
    "but the gradient function itself must be available in this file\n",
    "*/\n",
    "Eigen::MatrixXd sghmc(std::string gradU_choice, Eigen::MatrixXd eta, int niter, Eigen::MatrixXd alpha, Eigen::MatrixXd theta_0, Eigen::MatrixXd V_hat, Eigen::MatrixXd dat, int batch_size){\n",
    "    // Initialization and checks\n",
    "    int p = theta_0.rows(); // dimension of the thing you're sampling\n",
    "    int n = dat.rows();     // number of data observations\n",
    "    int p_dat = dat.cols(); // 2nd dimension of data\n",
    "    int nbatches = n / batch_size; // how many batches data will be broken into\n",
    "    // Set up dat_temp and dat_batch for use in loop below, as well as gradU_batch\n",
    "    Eigen::MatrixXd dat_temp = dat;\n",
    "    Eigen::MatrixXd dat_batch = Eigen::MatrixXd::Zero(batch_size, p_dat);\n",
    "    Eigen::MatrixXd gradU_batch = Eigen::MatrixXd::Zero(p, 1);\n",
    "    // set up matrix of 0s to hold samples\n",
    "    Eigen::MatrixXd theta_samps = Eigen::MatrixXd::Zero(p, niter*(n/batch_size));\n",
    "    // vector to hold indices for shuffling\n",
    "    std::vector<int> ind;\n",
    "    // fix beta_hat as described on pg. 6 of paper\n",
    "    Eigen::MatrixXd beta_hat = 0.5 * V_hat * eta;\n",
    "    // We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    // so this must be a positive definite matrix\n",
    "    Eigen::MatrixXd Sigma = 2.0 * (alpha - beta_hat) * eta;\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfSig(Sigma); // compute the Cholesky decomposition of Sigma\n",
    "    if(lltOfSig.info() == Eigen::NumericalIssue){ // check if we got error, and break out if so\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    Eigen::MatrixXd Sig_chol = lltOfSig.matrixL(); // get L in Chol decomp\n",
    "    if(batch_size > n){ // Need batch size to be <= the amount of data\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    // initialize more things! (nu and theta, to be specific)\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfeta(eta); // compute the Cholesky decomposition of eta\n",
    "    Eigen::MatrixXd eta_chol = lltOfeta.matrixL(); // get L in Chol decomp\n",
    "    Eigen::MatrixXd nu = eta_chol * rnorm_vec(p); // initialize nu\n",
    "    Eigen::MatrixXd theta = theta_0; // initialize theta \n",
    "    \n",
    "    // loop through algorithm to get niter*batch_size samples\n",
    "    int big_iter = 0;\n",
    "    for (int it=0; it<niter; it++) {\n",
    "        \n",
    "        // shuffle rows of dat to get dat_temp \n",
    "        Eigen::VectorXi indices = Eigen::VectorXi::LinSpaced(dat.rows(), 0, dat.rows());\n",
    "        std::random_shuffle(indices.data(), indices.data() + dat.rows());\n",
    "        dat_temp = indices.asPermutation() * dat;\n",
    "        \n",
    "        // Resample momentum every epoch\n",
    "        nu = eta_chol * rnorm_vec(p); // sample from MV normal\n",
    "        \n",
    "        // loop through the batches\n",
    "        int count_lower = 0;\n",
    "        int count_upper = batch_size;\n",
    "        for (int b=0; b<nbatches; b++){\n",
    "            int batch_ind = 0;\n",
    "            for (int ind_temp=count_lower; ind_temp<count_upper; ind_temp++){\n",
    "                dat_batch.row(batch_ind) = dat_temp.row(ind_temp);\n",
    "                batch_ind += 1;\n",
    "            }\n",
    "            count_lower += batch_size; // add batch size to each iterator\n",
    "            count_upper += batch_size;\n",
    "            if (gradU_choice == \"fig1\"){\n",
    "                gradU_batch = gradU_noisyFig1(theta);\n",
    "            } else if (gradU_choice == \"mixture_of_normals\"){\n",
    "                gradU_batch = gradU_mixNormals(theta, dat_batch, n, batch_size);\n",
    "            } else {\n",
    "                return theta_samps; // will just give back all-0s\n",
    "            }\n",
    "            nu = nu - eta * gradU_batch - alpha * nu + Sig_chol * rnorm_vec(p);\n",
    "            theta = theta + nu;\n",
    "            theta_samps.col(big_iter) = theta;\n",
    "            big_iter += 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return theta_samps;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(wrap, m) {\n",
    "    m.doc() = \"auto-compiled c++ extension\";\n",
    "    m.def(\"sghmc\", &sghmc);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_code.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_code.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "code = cppimport.imp(\"wrap\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    # Don't actually need 'data' in this example, just use\n",
    "    # it as a place-holder to fit into our function.\n",
    "    n = 100\n",
    "    x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "    # Set up start values and tuning params\n",
    "    theta_0 = np.array([0.0]) # Initialize theta\n",
    "    p = theta_0.shape[0]\n",
    "    eta = 0.001 * np.eye(p) # make this small\n",
    "    alpha = 0.01 * np.eye(p)\n",
    "    V = np.eye(p)\n",
    "    batch_size = n # since we're not actually using the data, don't need to batch it\n",
    "    niter = 50000 # Not too many iterations, just want to test for speed\n",
    "    # run SGHMC sampler\n",
    "    t0 = time.time()\n",
    "    samps_sghmc = code.sghmc(\"fig1\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # how long did it take?\n",
    "    print(\"Time taken with C++ version:\", t1 - t0, \"seconds\")\n",
    "    \n",
    "    ''' # ADD THIS to save plot to view\n",
    "    # plot the samples from the algorithm and save to a file\n",
    "    kdeplt = sns.kdeplot(samps_sghmc.reshape(-1))\n",
    "    fig = kdeplt.get_figure()\n",
    "    fig.savefig('Fig1_example.png')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with C++ version: 0.21611738204956055 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python test_code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "### Easy test example based on Figure 1 from the paper\n",
    "### See pg. 6 of Chen et. al. for their results/comparison\n",
    "\n",
    "# Log likelihood function\n",
    "def U(theta):\n",
    "    return(-2*theta**2 + theta**4)\n",
    "# True gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "# Noisy gradient, based on what they do in the paper for Fig 1\n",
    "def noisy_gradU(theta, x, n, batch_size):\n",
    "    '''Noisy gradient \\Delta\\tilde{U}(\\theta)=\\Delta U(\\theta)+N(0,4)\n",
    "    Extra args (x, n, batch_size) for compatibility with sghmc()'''\n",
    "    return -4*theta + 4*theta**3 + np.random.normal(0,2)\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Don't actually need 'data' in this example, just use\n",
    "# it as a place-holder to fit into our function.\n",
    "n = 100\n",
    "x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "# Set up start values and tuning params\n",
    "theta_0 = np.array([0.0]) # Initialize theta\n",
    "p = theta_0.shape[0]\n",
    "eta = 0.001 * np.eye(p) # make this small\n",
    "alpha = 0.01 * np.eye(p)\n",
    "V = np.eye(p)\n",
    "batch_size = n # since we're not actually using the data, don't need to batch it\n",
    "niter = 50000 # Lots of iterations\n",
    "\n",
    "# Below run SGHMC sampler with various implementations\n",
    "# NOTE may want to do like the %timeit magic does and loop over multiple test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with original python version: 23.926531314849854 seconds\n"
     ]
    }
   ],
   "source": [
    "# original \n",
    "t0 = time.time()\n",
    "samps_sghmc = sghmc(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "t1 = time.time()\n",
    "print(\"Time taken with original python version:\", t1 - t0, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with cleaned up python version: 3.5248875617980957 seconds\n"
     ]
    }
   ],
   "source": [
    "# cleaned\n",
    "t0 = time.time()\n",
    "samps_sghmc = sghmc_cleaned(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "t1 = time.time()\n",
    "print(\"Time taken with cleaned up python version:\", t1 - t0, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with Numba version: 6.474595785140991 seconds\n"
     ]
    }
   ],
   "source": [
    "# numba (maybe slower because overhead without gains since no data?)\n",
    "t0 = time.time()\n",
    "samps_sghmc = sghmc_numba(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "t1 = time.time()\n",
    "print(\"Time taken with Numba version:\", t1 - t0, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move on to using the C++ code to test the mixture of normals example, and compare it on time. Note that the way I have this written, the gradient function has to also be written in C++, which is a pain because we can't use the auto-gradient like we have in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.80991598]\n",
      "   [ 5.39404395]]]]\n",
      "[[ 3.80991598]\n",
      " [-5.39404395]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 100 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 50\n",
    "batch_size=20 # make this smallish\n",
    "\n",
    "# TEST MY GRADIENT HAND-CALCULATION \n",
    "def gradU_test(theta, x, n, batch_size):\n",
    "    grad = np.zeros((2,1))\n",
    "    for p in range(2):\n",
    "        star = 0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)\n",
    "        star_prime = 0.5 * np.exp(-0.5*(theta[p]-x)**2) * (theta[p]-x)\n",
    "        grad[p,0] = -theta[p]/10 - (n/batch_size)*np.sum(star_prime/star)\n",
    "    return grad\n",
    "# THESE SHOULD BE THE SAME!!\n",
    "print(gradU(theta, x, n, n))\n",
    "print(gradU_test(theta, x, n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like the gradient works. Now I'll actually check my C++ implementation and make sure it's sampling correctly, then time it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_code_mixture.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_code_mixture.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "code = cppimport.imp(\"wrap\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    ## Example #1:\n",
    "    ## Sampling from a mixture of normals in 1-D\n",
    "    ## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "    ## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "    # Set random seed\n",
    "    np.random.seed(1234)\n",
    "    # Set up the data\n",
    "    p = 2 #dimension of theta\n",
    "    theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "    n = 100 # smaller for test\n",
    "    x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "                  np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "    ## Initialize parameters and sample \n",
    "\n",
    "    # Initialize mean parameters\n",
    "    #theta_0 = np.random.normal(size=(p,1))\n",
    "    theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "    # Initialize tuning parameters:\n",
    "    # learning rate\n",
    "    eta = 0.01/n * np.eye(p)\n",
    "    # Friction rate\n",
    "    alpha = 0.1 * np.eye(p)\n",
    "\n",
    "    # Arbitrary guess at covariance of noise from mini-batching the data\n",
    "    V = np.eye(p)*1\n",
    "    niter = 50\n",
    "    batch_size=20 # make this smallish\n",
    "\n",
    "    # run SGHMC sampler\n",
    "    t0 = time.time()\n",
    "    niter = 1000\n",
    "    batch_size = n # just for testing\n",
    "    samps_sghmc = code.sghmc(\"mixture_of_normals\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    print(samps_sghmc)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # how long did it take?\n",
    "    print(\"Time taken with C++ version:\", t1 - t0, \"seconds\")\n",
    "    \n",
    "    '''# ADD THIS to save plot to view\n",
    "    # plot the samples from the algorithm and save to a file\n",
    "    kdeplt = sns.kdeplot(samps_sghmc[0,:], samps_sghmc[1,:]) # Plot the joint density\n",
    "    fig = kdeplt.get_figure()\n",
    "    fig.savefig('MixNormals_example.png')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.99080802 -2.99886303 -3.00786867 ...         nan         nan\n",
      "          nan]\n",
      " [ 3.00545214  3.01066057  3.0024139  ...         nan         nan\n",
      "          nan]]\n",
      "Time taken with C++ version: 0.022101879119873047 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python test_code_mixture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
