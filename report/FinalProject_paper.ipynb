{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python Implementation of Stochastic Gradient Hamiltonian Monte Carlo with example applications from simulated and real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "#### (INSTRUCTIONS ARE: 250 words or less. Identify 4-6 key phrases.)\n",
    "\n",
    "We implement the Stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm described in the paper *Stochastic Gradient Hamiltonian Monte Carlo* by Chen et al. (2014). This particular Hamiltonian Monte Carlo (HMC) sampling method allows for a sampling 'Big Data' through the use of a noisy gradient term calculated using minibatches of the full data set. We include python implementations of the algorithm; a description of the changes made and performance gains in each successive version may be found in the **Optimization** section of this document. We include a simulated example showing the performance of the algorithm in sampling the mean parameters of a two-component mixture of normals, and a real-world example showing **FIXME**. **FIXME** this needs more stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) sampling methods allow for proposing distant points in space with high acceptance probabilities. Hamiltonian Monte Carlo treats the probability density as a physical system being explored by a moving object. The object conserves energy overall, while trading between potential energy (in high probability regions) and momentum. HMC methods incorporate gradient information into the proposal distribution, which is especially advantageous for sampling a probability distribution with high correlations. This enables more efficient exploration of the space of interest than random walks, while still being in a Metropolis-Hastings framework.\n",
    "\n",
    "The paper *Stochastic Gradient Hamiltonian Monte Carlo* by Chen et al. (2014) develops a variant of HMC that sidesteps the computational limitation of HMC methods that comes from having to compute the gradient when the data are large. Specifically, the authors use a stochastic gradient and introduce a friction term that allows the algorithm to maintain the desired target distribution and invariance properties.\n",
    "\n",
    "This algorithm can be applied in any situation in which MCMC methods are needed, but is particularly useful in cases when the usual random walk methods tends to produce highly dependent samples (e.g., when there are high correlation between variables) or very low acceptance probabilities (e.g. when the sample space is very high dimensional). It also allows for scaling of Bayesian methods, with the use of minibatches of data.\n",
    "\n",
    "Other developments to improve the flexibility of HMC include the \"No U-Turn\" sampler and Riemann manifold HMC. The authors argue that their Stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm is an efficiency improvement to traditional HMC orthogonal to these other approaches, and thus could be combined with them to yield even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of algorithm\n",
    "\n",
    "The Stochastic Gradient HMC algorithm is complex in its derivation, but simple in implementation. In words, the algorithm begins when the sampler is initialized. The algorithm draws a specified number of discrete samples by moving around the targeted probability distribution. In standard HMC each move is a deterministic function of the current location and momentum. In SGHMC, random noise is added to account for the stochastic estimation of the target distribution. In addition, during each move a friction term reduces the momentum for stability. Computationally, the algorithm is:\n",
    "\n",
    "\n",
    "*Initialize ($\\theta_0$, $v_0$, $\\alpha$, $\\eta$, $\\hat{\\beta}$)*\n",
    "\n",
    "**for t = 1,2,... :**\n",
    "    \n",
    "$\\qquad \\theta_i = \\theta_{i-1} + v$\n",
    "    \n",
    "$\\qquad v = v - \\eta \\nabla \\tilde{U(}x) - \\alpha v + N(0, 2(\\alpha - \\hat{\\beta}))$\n",
    "\n",
    "\n",
    "**end **\n",
    "\n",
    "The above computational algorithm represents the Hamiltonian dynamics, re-expressed in terms more familiar to Stochastic Gradient Descent. $\\tilde{U}(x)$ is the minibatch approximation to the gradient, $\\eta$ is the learning rate, $\\alpha$ is the friction constant, and $\\hat{\\beta}$ is the approximation to the noise introduced by the stochastic gradient.\n",
    "\n",
    "To go into more detail on how we perform mini-batching, the full data are split into `nbatches` random batches of size `batch_size` (defined by the user), at each iteration of the sampler. If there is not an even way to split the full data, the extra observations are thrown away in that epoch. The sampler runs on each batch, updating after each, until all batches have been explored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe optimization for performance\n",
    "\n",
    "Vectorization was performed in the original algorithm, so we don't have any optimization results for vectorized vs. non-vectorized Python versions of the sampler. Things we did to optimize performance relative to our original algorithm (see optimization_work.ipynb) include:\n",
    "1. An algorithmic performance improvement in multivariate normal sampling. We obtain improvement through the use of Cholesky decomposition based multivariate normal sampling with pre-computation (outside of the main sampler loop) of the Cholesky decomposition of covariance matrices needed to sample from the relevant multivariate normals.\n",
    "2. JIT compilation of our main SGHMC sampling algorithm (we tried the data batching code with and without JIT compilation for comparison within the JIT-compiled algorithm).\n",
    "3. Re-writing the main algorithm, data batching, and gradient functions in C++ and using pybind11 to wrap them.\n",
    "\n",
    "|                            | Original     | Efficiency improvements | Numpy version (main algorithm only) | Numpy version (main algorithm and data batching) | C++ version   |\n",
    "|----------------------------|--------------|-------------------------|-------------------------------------|--------------------------------------------------|---------------|\n",
    "| Figure 1 Example           | 23.5 (0.297) | 3.52 (0.091)            | 4.81 (0.465)                        |                                                  | 0.197 (0.003) |\n",
    "| Mixture of Normals Example | 22.7 (0.144) | 21.7 (0.253)            | 22.0 (0.176)                        | 22.1 (0.062)                                     | 0.019 (0.000) |\n",
    "\n",
    "Table 1 gives the time taken for the sampler to draw 50,000 samples from the target distribution (Figure 1 example) or 2,000 samples from the target (with 500 iterations over 4 size-50 data batches, in the mixtures of normals example). Times shown are the mean (standard deviation) across 7 runs in seconds. Tuning parameters are chosen to be the same as those described in the **Applications to simulated data sets** section below. Note that the deteriorated performance of the JIT-compiled version in the case of the Figure 1 example may be due to the fact that the data are not being resampled here (rather, artificial noise is being injected), so the overhead of JIT may outweigh the benefits. Since data batching are not performed, the second numpy version is the same as the first. Note that the C++ version of the mixture of normals example yielded unstable behavior so it is possible we have a small bug in the data batching section of the C++ code, but computation times should still be relevant.\n",
    "\n",
    "We see that the C++ version of the code is 11,839% faster (calculated as (old - new) / new x 100%) than the original version for the Figure 1 example and 118,308% faster than the original for the mixtures of normals example. Profiling showed expensive computation times for the auto-gradient calculation done by the `jacobian` function we used from the `autograd` package in the Python version of the code. The JIT-compiled version was not able to improve as much relative to the original, likely because it could not automatically C++-ify these expensive computations, whereas explicitly coding the algorithm in C++ allowed us to write the gradient functions by hand.\n",
    "\n",
    "If we were working with extremely large data sets, distributed computing would be an optimization avenue to explore. However, we used a small enough data example that local computation was sufficient.\n",
    "\n",
    "As the algorithm is a MCMC sampler, parallelization is not possible because we have dependence within each chain. An option could be to run multiple chains and parallelize over the number of chains, but this would only give gains up to the number of chains run rather than improving the speed of the underlying algorithm itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to simulated data sets\n",
    "\n",
    "We include two simulated examples with known target distributions from which the sampler should be drawing. The first, from the Chen et al. paper, samples from the target distribution with $U(\\theta) = -2\\theta^2 + \\theta^4.$ Figure 1 shows the true distribution of $\\theta,$ which is available in closed form.\n",
    "\n",
    "![EMPTY_TEXT](Example1_truth.png \"TITLE\")\n",
    "<center>Figure 1: True distribution of $\\theta$</center>\n",
    "\n",
    "For the SGHMC we use artificially injected gradient noise; specifically, we set $\\Delta \\tilde{U}(\\theta)=\\Delta U(\\theta) + \\mathcal{N}(0,4).$ We set $\\eta=0.001, \\alpha=0.01, V=1,$ and sample for $50,000,000$ iterations using the C++ implementation of the algorithm. We see that in Figure 2a that our sampler finds the true modes of the posterior, but seems to concentrate too closely about these modes; the density is more peaked than the truth. We believe this phenomenon may be due to our choice of tuning parameters, but fiddling with these values often led to algorithm instability so we will leave this performance failure to be solved in future work. We note that the authors of the paper had success sampling from this example, so it is our implementation is flawed rather than the algorithm.\n",
    "\n",
    "![EMPTY_TEXT](Example1_a.png \"TITLE\")\n",
    "<center>Figure 2a: Univariate $\\theta$ sampling with SGHMC</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example samples the mean parameters of a mixture of normals distribution. We assume we have data $x$ with $x \\sim 0.5 \\mathcal{N}(\\mu_1,1) + 0.5 \\mathcal{N}(\\mu_2,1),$ i.e. a two-component mixture model with known variances and unknown means. We simulate $n=10,000$ data points drawn from each mixture component. We set the priors on $\\mu_1$ and $\\mu_2$ both at independent $\\mathcal{N}(0,10).$ We set $\\eta=\\frac{0.01}{n} \\mathcal{I}, \\alpha=0.1 \\mathcal{I}, V=\\mathcal{I},$ the batch size to 1,000 and sample for $100$ iterations using the Python implementation of the algorithm. The results are shown below in Figure 2b. Note that, as in our previous example, the algorithm seems to be focusing too precisely on the mode of the posterior, enough so that we are actually seeming to focus away from the MLE, even with a relatively diffuse prior. Also note that our algorithm hasn't picked up on the fact that we have two modes (due to the label swapping problem of our mixture model), likely because it doesn't stray from its mode.\n",
    "\n",
    "![EMPTY_TEXT](MixNorm_a.png \"TITLE\")\n",
    "<center>Figure 2b: Mixtures of normals sampling with SGHMC</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to real data sets\n",
    "\n",
    "**Test the algorithm on the real-world examples in the orignal paper if possible. Try to find at least one other real-world data set not in the original paper and test it on that. Describe and interpret the results.**\n",
    "\n",
    "We still need to pick a real data set (maybe some kind of regression data set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative analysis with competing algorihtms\n",
    "\n",
    "We compare the SGHMC method (as coded by us) to both the standard HMC method as implemented in the package `pyhmc`, and to Stan (a no-U-turn implementation of HMC) through the `pystan` package.\n",
    "\n",
    "The results for the mixture of normals models are shown below. The `pyhmc` sampler exhibited difficulties even with these relatively simple examples. For the univariate sampling of $\\theta$ in Example 1, we find that the sampler diverges and exhibits unstable behavior (the results below are for 500,000 samples). For the mixtures of normals example, if we begin the sampler at the true mixture component means, -3 and 3, we find that the algorithm does not explore the space at all; that is, we just get repeated samples of -3 and 3 as the output! When initialized elsewhere error messages were returned. Also `pyhmc` is a very slow algorithm; 100 samples of Example  took **FIXME** time to run. I would not choose to use this package again.\n",
    "\n",
    "![EMPTY_TEXT](Example1_b.png \"TITLE\")\n",
    "<center>Figure 2a: Univariate $\\theta$ sampling with SGHMC</center>\n",
    "\n",
    "![EMPTY_TEXT](MixNorm_b.png \"TITLE\")\n",
    "<center>Figure 3a: Mixtures of normals sampling with `pyhmc`</center>\n",
    "\n",
    "For Stan, we do not choose an initial point, but let the package run using its defaults. We set the number of chains to 4 and the number of iterations to 1000, with the default number of burn-in samples. The most notable difference between our results and Stan is that Stan finds both modes of the posterior (we have a label-swapping problem with our mixture model, which we don't address in our model formulation), and the credible intervals are much more realistic from Stan. Since Stan is a trusted, well-written and vetted algorithm; the sampled posterior is much closer to the true posterior than our algorithm, and without the need to fiddle with finicky tuning parameters.\n",
    "\n",
    "![EMPTY_TEXT](MixNorm_c.png \"TITLE\")\n",
    "<center>Figure 3bc: Mixtures of normals sampling with Stan</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion/conclusion\n",
    "\n",
    "The strengths and weaknesses of the SGHMC algorithm have become very apparent through the implementation and testing process. The SGHMC algorithm is an extremely efficient sampler, as evidenced from testing. Optimizing the code has produced incredibly quick results on simulated datasets. However, the sampler includes a number of sensitive hyper parameters - the learning rate, the friction, and the estimation for the noise added due to stochastic minibatch sampling. If these are set incorrectly then the sampler is unstable. In simulated examples where the final answer is known, we have been successful in tuning these hyperparameters to create a robust sampler. However, in real-world situations where the truth is unknown, the basic SGHMC algorithm is insufficiently robust to be trusted. This matches the results that we have seen with the basic HMC package. Just using the default settings often produces unstable samples.\n",
    "\n",
    "However, the highly developed STAN package uses the No U-Turn Sampler (NUTS) HMC algorithm, which is both efficient and stable without hyperparameter tuning. As noted in the original paper itself, there are a number of adaptive improvements that have been made to HMC - NUTS being one of them - which could be applied to the SGHMC algorithm. In fact, papers have already been written which fuse these ideas, such as in *A Complete Recipe for Stochastic Gradient MCMC*, which includes an algorithm for stochastic gradient Riemann HMC. As these improvements are incorporated, the efficiencies of SGHMC on large datasets will be more practical for real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References/bibliography\n",
    "\n",
    "Chen, T., Fox, E., and Guestrin, C. *Stochastic Gradient Hamiltonian Monte Carlo*. ArXiv e-prints. 1402.4102.\n",
    "\n",
    "Ma Y.A., Chen, T., and Fox, E. *A Complete Recipe for Stochastic Gradient MCMC*. ArXiv e-prints. 1506.04696.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The code should be in a public GitHub repository with:\n",
    "\n",
    "1. A README file\n",
    "2. An open source license\n",
    "3. Source code\n",
    "4. Test code\n",
    "5. Examples\n",
    "6. A reproducible report\n",
    "\n",
    "The package should be downloadable and installable with `python setup.py install`, or even posted to PyPI adn installable with `pip install package`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Each item is worth 10 points, but some sections will give up to 10 bonus points if done really well. Note that the \"difficulty factor\" of the chosen algorithm will be factored into the grading. \n",
    "\n",
    "1. Is the abstract, background and discussion readable and clear? (10-20 points)\n",
    "2. Is the algorithm description clear and accurate? (10-20 points)\n",
    "3. Has the algorihtm been optimized? (10-20 points)\n",
    "4. Are the applicaitons to simulated/real data clear and useful? (10-20 points)\n",
    "5. Was the comarative analysis done well? (10-20 points points)\n",
    "6. Is there a well-maitnatined Github repository for the code? (10 points)\n",
    "7. Is the document show evidenc of literate programming? (10 points)\n",
    "8. Is the analyiss reproducible? (10 points)\n",
    "9. Is the code tested? Are examples provided? (10 points)\n",
    "10. Is the package easily installable? (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
