#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr 22 21:14:14 2018

@author: isaaclavine
"""

### Using a dataset from sci kit learn: Boston (home prices)

### Will perform a linear regression, and learn the coefficients

from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import autograd.numpy as np
from autograd import jacobian
import seaborn as sns

boston = load_boston()
x = boston.data
# 3 predictors are crime rate, avg # of rooms, and percent of poor ppl
x = boston.data[:,[0,5,12]] 
# outcome is the median home price in a neighborhood
y = boston.target 

## Linear Model: Y = X*BETA

# UNINFORMATIVE PRIOR: 
# BETA: Independent normals, centered at 0, large variance
# p(sigma^2) = constant - improper prior
def log_prior(theta):
    beta = theta[1:]
    return(-(1/(2*1000))*beta.T@beta)
    

# LOG-LIKELIHOOD:      
def log_lik(theta, y, x, n):
    beta = theta[1:]
    sigmasq = theta[0]
    SSE = (y - x @ beta).T @ (y-x @ beta)
    return(-n/2*np.log(sigmasq) - SSE/(2*sigmasq))

def U(theta, data, n, batch_size):
    y = data[:,0].reshape(-1,1)
    x = data[:,1:]
    return(-log_prior(theta) - (n/batch_size)*log_lik(theta, y, x, n))
    
gradU = jacobian(U, argnum=0)

# Here's the least squares solution from numpy
beta, res, rank, s = np.linalg.lstsq(x, y)

# beta = -0.098, 4.86, -0.606

np.random.seed(1234)
# Set up the data
n, p = x.shape
p += 1 # Adding in the variance term
# Initialize the sampler at the MLE
theta_0 = np.array([res/(n-p), -0.098, 4.86, -.606], dtype=float).reshape(-1,1)

# Set the sampling hyperparameters
# learning rate
eta = 0.01/n * np.eye(p)
# Friction rate
alpha = 0.1 * np.eye(p)

V = np.eye(p)
niter = 1000
batch_size=100

data = np.c_[y,x]

samps = sghmc(gradU, eta, niter, alpha, theta_0, V, data, batch_size)

sns.kdeplot(samps[0,:]) # MLE is 30.1
sns.kdeplot(samps[1,:]) # MLE is -.098
sns.kdeplot(samps[2,:]) # MLE is 4.86
sns.kdeplot(samps[3,:]) # MLE is -.606

