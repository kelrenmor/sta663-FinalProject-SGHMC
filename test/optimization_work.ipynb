{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## An optimization test bed.\n",
    "\n",
    "First, I'll define the usual functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(X):\n",
    "    '''Check whether a matrix X is pos definite.\n",
    "    Returns True or False, depending on X.\n",
    "    '''\n",
    "    return np.all(np.linalg.eigvals(X) > 0)\n",
    "\n",
    "def batch_data(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)\n",
    "\n",
    "def sghmc(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import some optimization things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just look at optimization for the mixture of normals problem. I don't care about performance of the sampler itself right now, so I turn down the size of n and niter just to make it go relatively quickly in my tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 100 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 50\n",
    "batch_size=20 # make this smallish\n",
    "\n",
    "# Don't actually run sampling algorithm here\n",
    "# samps = sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cleaning up the multivariate normal sampling. Pre-calculate the Cholesky decompositions, and use the Cholesky-based sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just try throwing `@jit` in front of things and seeing how much improvement we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_data_numba(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data_numba(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the batch_data_numba version won't actually help things... It may just add overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba2(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size) # use original batch_data\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the times for the above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 s ± 26.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 s ± 18 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 s ± 10.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data_numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.51 s ± 9.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step should be to do something more fancy, like maybe parallelize the batch function loop or convert things to C++/Cython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
