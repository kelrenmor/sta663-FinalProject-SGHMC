{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## An optimization test bed.\n",
    "\n",
    "First, I'll define the usual functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos_def(X):\n",
    "    '''Check whether a matrix X is pos definite.\n",
    "    Returns True or False, depending on X.\n",
    "    '''\n",
    "    return np.all(np.linalg.eigvals(X) > 0)\n",
    "\n",
    "def batch_data(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)\n",
    "\n",
    "def sghmc(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > dat.shape[0]): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 np.random.multivariate_normal(np.zeros(p), Sigma).reshape(p, -1)\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import some optimization things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just look at optimization for the mixture of normals problem. I don't care about performance of the sampler itself right now, so I turn down the size of n and niter just to make it go relatively quickly in my tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 200 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 500\n",
    "batch_size=50 # make this smallish\n",
    "\n",
    "# Don't actually run sampling algorithm here\n",
    "# samps = sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try cleaning up the multivariate normal sampling. Pre-calculate the Cholesky decompositions, and use the Cholesky-based sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just try throwing `@jit` in front of things and seeing how much improvement we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_data_numba(data, batch_size):\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_batches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_batches)\n",
    "    return(data, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data_numba(dat, batch_size)\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the batch_data_numba version won't actually help things... It may just add overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba2(gradU, eta, niter, alpha, theta_0, V_hat, dat, batch_size):\n",
    "    '''Define SGHMC as described in the paper\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin \n",
    "    Stochastic Gradient Hamiltonian Monte Carlo \n",
    "    ICML 2014.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eta = eps^2 M^(-1)\n",
    "    niter = number of samples to generate\n",
    "    alpha = eps M^(-1) C\n",
    "    theta_0 = initial val of parameter(s) to be sampled\n",
    "    V_hat = estimated covariance matrix of stoch grad noise\n",
    "    See paper for more details\n",
    "\n",
    "    The return is:\n",
    "    A np.array of positions of theta.'''\n",
    "\n",
    "    ### Initialization and checks ###\n",
    "    # get dimension of the thing you're sampling\n",
    "    p = len(theta_0)\n",
    "    # set up matrix of 0s to hold samples\n",
    "    n = dat.shape[0]\n",
    "    theta_samps = np.zeros((p, niter*(n // batch_size)))\n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = 0.5 * V_hat @ eta\n",
    "    # We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    # so this must be a positive definite matrix\n",
    "    Sigma = 2 * (alpha - beta_hat) @ eta\n",
    "    Sig_chol = np.linalg.cholesky(Sigma)\n",
    "    if not is_pos_def( Sigma ): \n",
    "        print(\"Error: (alpha - beta_hat) eta not pos def\")\n",
    "        return\n",
    "    # Need batch size to be <= the amount of data\n",
    "    if (batch_size > n): \n",
    "        print(\"Error: batch_size must be <= number of data points\")\n",
    "        return\n",
    "\n",
    "    # initialize nu and theta \n",
    "    nu = np.random.multivariate_normal(np.zeros(p), eta).reshape(p,-1)\n",
    "    theta = theta_0\n",
    "    \n",
    "    # set up for Chol decomp for MV normal sampling of nu every epoch\n",
    "    eta_chol = np.linalg.cholesky(eta)\n",
    "    \n",
    "    # loop through algorithm to get niter samples\n",
    "    it = 0\n",
    "    for i in range(niter):\n",
    "        dat_resh, nbatches = batch_data(dat, batch_size) # use original batch_data\n",
    "        \n",
    "        # Resample momentum every epoch\n",
    "        nu = eta_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "        \n",
    "        for batch in range(nbatches):\n",
    "            gradU_batch = gradU(theta, dat_resh[:,:,batch], n, batch_size).reshape(p,-1)\n",
    "            nu = nu - eta @ gradU_batch - alpha @ nu + \\\n",
    "                 Sig_chol @ np.random.normal(size=p).reshape(p,-1) # sample from MV normal\n",
    "            theta = theta + nu\n",
    "            theta_samps[:,it] = theta.reshape(-1,p)\n",
    "            it = it + 1\n",
    "        \n",
    "    return theta_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the times for the above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.8 s ± 473 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4 s ± 92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_cleaned(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit sghmc_numba(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data_numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sghmc_numba2(gradU, eta, niter, alpha, theta_0, V, x, batch_size) # use batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step should be to do something more fancy, like maybe convert things to C++/Cython. You can't really parallelize the outer epoch loop or the batch function loop, because there is dependency in the Markov chain.\n",
    "\n",
    "Below, I write the C++ code and test it using Fig 1 (the toy example from the paper). Note that the way I have this written, the gradient function has to also be written in C++, which is a pain because we can't use the auto-gradient like we have in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE: Unused things that I didn't end up keeping but want to save just in case are in this block.\n",
    "\n",
    "// checks whether a matrix X is pos definite, returns bool type\n",
    "bool not_is_pos_def(Eigen::MatrixXd xs){\n",
    "    bool res = false; // assume xs is not pos definite\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfX(xs); // compute the Cholesky decomposition of xs\n",
    "    if(lltOfX.info() != Eigen::NumericalIssue){res = true;} // check if we did not get error  \n",
    "    return res;\n",
    "}\n",
    "\n",
    "// get gradient for mixture of normals example\n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    int p = theta.rows();\n",
    "    Eigen::MatrixXd star = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd star_prime0 = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd star_prime1 = Eigen::MatrixXd::Zero(n, 1);\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<n; i++) {\n",
    "        star(i,0) = 0.5*(-0.5*(theta(0,0)-x(i,0)).pow(2)).exp() + 0.5*(-0.5*(theta(1,0)-x(i,0)).pow(2)).exp();\n",
    "        star_prime0(i,0) = 0.5*(-0.5*(theta(0,0)-x(i,0)).pow(2)).exp()*(theta(0,0)-x(i,0));\n",
    "        star_prime1(i,0) = 0.5*(-0.5*(theta(1,0)-x(i,0)).pow(2)).exp()*(theta(1,0)-x(i,0));\n",
    "    }\n",
    "    grad(0,0) = -theta(0,0)/10 - (n/batch_size)*(star_prime0.array()/star.array()).sum();\n",
    "    grad(1,0) = -theta(1,0)/10 - (n/batch_size)*(star_prime1.array()/star.array()).sum();\n",
    "    return grad;\n",
    "}\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrap.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file wrap.cpp\n",
    "<%\n",
    "cfg['compiler_args'] = ['-std=c++11']\n",
    "cfg['include_dirs'] = ['../../eigen3']\n",
    "setup_pybind11(cfg)\n",
    "%>\n",
    "\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/eigen.h>\n",
    "#include <stdexcept>\n",
    "#include <algorithm> // std::random_shuffle\n",
    "#include <random>\n",
    "\n",
    "#include <Eigen/LU>\n",
    "#include <Eigen/Dense>\n",
    "\n",
    "namespace py = pybind11;\n",
    "using std::default_random_engine;\n",
    "using std::normal_distribution;\n",
    "        \n",
    "// start random number engine with fixed seed\n",
    "default_random_engine re{1234};\n",
    "// set up random normal rnorm to work like in python\n",
    "normal_distribution<double> norm(0, 1); // mean and standard deviation\n",
    "auto rnorm = bind(norm, re);\n",
    "\n",
    "// fill xs with draws from N(0,1) and return this n x 1 dim vector\n",
    "Eigen::MatrixXd rnorm_vec(int n) {\n",
    "    Eigen::MatrixXd xs = Eigen::MatrixXd::Zero(n, 1);\n",
    "    for (int i=0; i<n; i++) {xs(i,0) = rnorm();}\n",
    "    return xs;\n",
    "}\n",
    "    \n",
    "// get noisy gradient of Fig1 example from Chen et. al. paper\n",
    "Eigen::MatrixXd gradU_noisyFig1(Eigen::MatrixXd theta) {\n",
    "    Eigen::MatrixXd xs = -4*theta.array() + 4*theta.array().pow(3) + 2*rnorm_vec(theta.rows()).array();\n",
    "    return xs;\n",
    "} \n",
    "    \n",
    "// get gradient for mixture of normals example\n",
    "Eigen::MatrixXd gradU_mixNormals(Eigen::MatrixXd theta, Eigen::MatrixXd x, int n, int batch_size) {\n",
    "    int p = theta.rows();\n",
    "    Eigen::ArrayXd c_0 = theta(0,0) - x.array();\n",
    "    Eigen::ArrayXd c_1 = theta(1,0) - x.array();\n",
    "    Eigen::ArrayXd star = 0.5 * (-0.5 * c_0.pow(2)).exp() + 0.5 * (-0.5 * c_1.pow(2)).exp();\n",
    "    Eigen::ArrayXd star_prime;\n",
    "    Eigen::MatrixXd grad = Eigen::MatrixXd::Zero(p, 1);\n",
    "    for (int i=0; i<p; i++) {\n",
    "        star_prime = 0.5 * (-0.5 * (theta(i,0) - x.array()).pow(2)).exp() * (theta(i,0) - x.array());\n",
    "        grad(i,0) = -theta(i,0)/10 - (n/batch_size)*(star_prime/star).sum();\n",
    "    }\n",
    "    return grad;\n",
    "} \n",
    "\n",
    "/* \n",
    "SGHMC algorithm as described in the paper by Chen et al.\n",
    "Note that users specify the choice of which gradient function to use by gradU_choice, \n",
    "but the gradient function itself must be available in this file\n",
    "*/\n",
    "Eigen::MatrixXd sghmc(std::string gradU_choice, Eigen::MatrixXd eta, int niter, Eigen::MatrixXd alpha, Eigen::MatrixXd theta_0, Eigen::MatrixXd V_hat, Eigen::MatrixXd dat, int batch_size){\n",
    "    // Initialization and checks\n",
    "    int p = theta_0.rows(); // dimension of the thing you're sampling\n",
    "    int n = dat.rows();     // number of data observations\n",
    "    int p_dat = dat.cols(); // 2nd dimension of data\n",
    "    int nbatches = n / batch_size; // how many batches data will be broken into\n",
    "    // Set up dat_temp and dat_batch for use in loop below, as well as gradU_batch\n",
    "    Eigen::MatrixXd dat_temp = dat;\n",
    "    Eigen::MatrixXd dat_batch = Eigen::MatrixXd::Zero(batch_size, p_dat);\n",
    "    Eigen::MatrixXd gradU_batch = Eigen::MatrixXd::Zero(p, 1);\n",
    "    // set up matrix of 0s to hold samples\n",
    "    Eigen::MatrixXd theta_samps = Eigen::MatrixXd::Zero(p, niter*(n/batch_size));\n",
    "    // vector to hold indices for shuffling\n",
    "    std::vector<int> ind;\n",
    "    // fix beta_hat as described on pg. 6 of paper\n",
    "    Eigen::MatrixXd beta_hat = 0.5 * V_hat * eta;\n",
    "    // We're sampling from a N(0, 2(alpha - beta_hat) @ eta)\n",
    "    // so this must be a positive definite matrix\n",
    "    Eigen::MatrixXd Sigma = 2.0 * (alpha - beta_hat) * eta;\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfSig(Sigma); // compute the Cholesky decomposition of Sigma\n",
    "    if(lltOfSig.info() == Eigen::NumericalIssue){ // check if we got error, and break out if so\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    Eigen::MatrixXd Sig_chol = lltOfSig.matrixL(); // get L in Chol decomp\n",
    "    if(batch_size > n){ // Need batch size to be <= the amount of data\n",
    "        return theta_samps; // will just give back all-0s\n",
    "    }\n",
    "    // initialize more things! (nu and theta, to be specific)\n",
    "    Eigen::LLT<Eigen::MatrixXd> lltOfeta(eta); // compute the Cholesky decomposition of eta\n",
    "    Eigen::MatrixXd eta_chol = lltOfeta.matrixL(); // get L in Chol decomp\n",
    "    Eigen::MatrixXd nu = eta_chol * rnorm_vec(p); // initialize nu\n",
    "    Eigen::MatrixXd theta = theta_0; // initialize theta \n",
    "    \n",
    "    // loop through algorithm to get niter*batch_size samples\n",
    "    int big_iter = 0;\n",
    "    for (int it=0; it<niter; it++) {\n",
    "        \n",
    "        // shuffle rows of dat to get dat_temp \n",
    "        Eigen::VectorXi indices = Eigen::VectorXi::LinSpaced(dat.rows(), 0, dat.rows());\n",
    "        std::random_shuffle(indices.data(), indices.data() + dat.rows());\n",
    "        dat_temp = indices.asPermutation() * dat;\n",
    "        \n",
    "        // Resample momentum every epoch\n",
    "        nu = eta_chol * rnorm_vec(p); // sample from MV normal\n",
    "        \n",
    "        // loop through the batches\n",
    "        int count_lower = 0;\n",
    "        int count_upper = batch_size;\n",
    "        for (int b=0; b<nbatches; b++){\n",
    "            int batch_ind = 0;\n",
    "            for (int ind_temp=count_lower; ind_temp<count_upper; ind_temp++){\n",
    "                dat_batch.row(batch_ind) = dat_temp.row(ind_temp);\n",
    "                batch_ind += 1;\n",
    "            }\n",
    "            count_lower += batch_size; // add batch size to each iterator\n",
    "            count_upper += batch_size;\n",
    "            if (gradU_choice == \"fig1\"){\n",
    "                gradU_batch = gradU_noisyFig1(theta);\n",
    "            } else if (gradU_choice == \"mixture_of_normals\"){\n",
    "                gradU_batch = gradU_mixNormals(theta, dat_batch, n, batch_size);\n",
    "            } else {\n",
    "                return theta_samps; // will just give back all-0s\n",
    "            }\n",
    "            nu = nu - eta * gradU_batch - alpha * nu + Sig_chol * rnorm_vec(p);\n",
    "            theta = theta + nu;\n",
    "            theta_samps.col(big_iter) = theta;\n",
    "            big_iter += 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return theta_samps;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(wrap, m) {\n",
    "    m.doc() = \"auto-compiled c++ extension\";\n",
    "    m.def(\"sghmc\", &sghmc);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with the Fig1 example. First, make a figure to use in our report, then time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make_fig1_code.py\n"
     ]
    }
   ],
   "source": [
    "%%file make_fig1_code.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "code = cppimport.imp(\"wrap\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    # Don't actually need 'data' in this example, just use\n",
    "    # it as a place-holder to fit into our function.\n",
    "    n = 100\n",
    "    x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "    # Set up start values and tuning params\n",
    "    theta_0 = np.array([0.0]) # Initialize theta\n",
    "    p = theta_0.shape[0]\n",
    "    eta = 0.001 * np.eye(p) # make this small\n",
    "    alpha = 0.01 * np.eye(p)\n",
    "    V = np.eye(p)\n",
    "    batch_size = n # since we're not actually using the data, don't need to batch it\n",
    "    niter = 50000000 # TONS of iterations, to get nice figure\n",
    "    # run SGHMC sampler\n",
    "    samps_sghmc = code.sghmc(\"fig1\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    # save to a file\n",
    "    np.save(\"samps_sghmc.npy\",samps_sghmc)\n",
    "    # ADD THIS to save plot to view\n",
    "    # plot the samples from the algorithm and save to a file\n",
    "    kdeplt = sns.kdeplot(samps_sghmc.reshape(-1))\n",
    "    fig = kdeplt.get_figure()\n",
    "    fig.savefig('Example1_a.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "In file included from /home/jovyan/work/sta-663-2018/project/.rendered.wrap.cpp:4:0:\n",
      "/opt/conda/include/python3.6m/pybind11/eigen.h:31:22: fatal error: Eigen/Core: No such file or directory\n",
      " #include <Eigen/Core>\n",
      "                      ^\n",
      "compilation terminated.\n",
      "error: command 'gcc' failed with exit status 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python make_fig1_code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_code.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_code.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "code = cppimport.imp(\"wrap\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    # Don't actually need 'data' in this example, just use\n",
    "    # it as a place-holder to fit into our function.\n",
    "    n = 100\n",
    "    x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "    # Set up start values and tuning params\n",
    "    theta_0 = np.array([0.0]) # Initialize theta\n",
    "    p = theta_0.shape[0]\n",
    "    eta = 0.001 * np.eye(p) # make this small\n",
    "    alpha = 0.01 * np.eye(p)\n",
    "    V = np.eye(p)\n",
    "    batch_size = n # since we're not actually using the data, don't need to batch it\n",
    "    niter = 50000 # Not too many iterations, just want to test for speed\n",
    "    \n",
    "    times_all = np.zeros(7)\n",
    "    for i in range(7):\n",
    "        # run SGHMC sampler\n",
    "        t0 = time.time()\n",
    "        samps_sghmc = code.sghmc(\"fig1\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "        t1 = time.time()\n",
    "        times_all[i] = t1 - t0\n",
    "    print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "          \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19707223347255162 s ± 0.0027052957875769016 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python test_code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "### Easy test example based on Figure 1 from the paper\n",
    "### See pg. 6 of Chen et. al. for their results/comparison\n",
    "\n",
    "# Log likelihood function\n",
    "def U(theta):\n",
    "    return(-2*theta**2 + theta**4)\n",
    "# True gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "# Noisy gradient, based on what they do in the paper for Fig 1\n",
    "def noisy_gradU(theta, x, n, batch_size):\n",
    "    '''Noisy gradient \\Delta\\tilde{U}(\\theta)=\\Delta U(\\theta)+N(0,4)\n",
    "    Extra args (x, n, batch_size) for compatibility with sghmc()'''\n",
    "    return -4*theta + 4*theta**3 + np.random.normal(0,2)\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Don't actually need 'data' in this example, just use\n",
    "# it as a place-holder to fit into our function.\n",
    "n = 100\n",
    "x = np.array([np.random.normal(0, 1, (n,1))]).reshape(-1,1)\n",
    "# Set up start values and tuning params\n",
    "theta_0 = np.array([0.0]) # Initialize theta\n",
    "p = theta_0.shape[0]\n",
    "eta = 0.001 * np.eye(p) # make this small\n",
    "alpha = 0.01 * np.eye(p)\n",
    "V = np.eye(p)\n",
    "batch_size = n # since we're not actually using the data, don't need to batch it\n",
    "niter = 50000 # Not too many iterations, just want to test for speed\n",
    "\n",
    "# Below run SGHMC sampler with various implementations\n",
    "# NOTE may want to do like the %timeit magic does and loop over multiple test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.51887375967843 s ± 0.2969187075750739 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    # original \n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.523411682673863 s ± 0.09111114957236507 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    # cleaned\n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc_cleaned(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.81162885257176 s ± 0.4652202205816858 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "times_all = np.zeros(7)\n",
    "for i in range(7):\n",
    "    # numba (maybe slower because overhead without gains since no data?)\n",
    "    t0 = time.time()\n",
    "    samps_sghmc = sghmc_numba(noisy_gradU, eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "    t1 = time.time()\n",
    "    times_all[i] = t1 - t0\n",
    "print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "      \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move on to using the C++ code to test the mixture of normals example, and compare it on time. Note that the way I have this written, the gradient function has to also be written in C++, which is a pain because we can't use the auto-gradient like we have in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.80991598]\n",
      "   [ 5.39404395]]]]\n",
      "[[ 3.80991598]\n",
      " [-5.39404395]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 100 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 50\n",
    "batch_size=20 # make this smallish\n",
    "\n",
    "# TEST MY GRADIENT HAND-CALCULATION \n",
    "def gradU_test(theta, x, n, batch_size):\n",
    "    grad = np.zeros((2,1))\n",
    "    for p in range(2):\n",
    "        star = 0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)\n",
    "        star_prime = 0.5 * np.exp(-0.5*(theta[p]-x)**2) * (theta[p]-x)\n",
    "        grad[p,0] = -theta[p]/10 - (n/batch_size)*np.sum(star_prime/star)\n",
    "    return grad\n",
    "# THESE SHOULD BE THE SAME!!\n",
    "print(gradU(theta, x, n, n))\n",
    "print(gradU_test(theta, x, n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like the gradient works. Now I'll actually check my C++ implementation and make sure it's sampling correctly, then time it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_code_mixture.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_code_mixture.py\n",
    "import cppimport\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "code = cppimport.imp(\"wrap\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1234)\n",
    "    ## Example #1:\n",
    "    ## Sampling from a mixture of normals in 1-D\n",
    "    ## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "    ## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "    # Set random seed\n",
    "    np.random.seed(1234)\n",
    "    # Set up the data\n",
    "    p = 2 #dimension of theta\n",
    "    theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "    n = 200 # smaller for test\n",
    "    x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "                  np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "    ## Initialize parameters and sample \n",
    "\n",
    "    # Initialize mean parameters\n",
    "    #theta_0 = np.random.normal(size=(p,1))\n",
    "    theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "    # Initialize tuning parameters:\n",
    "    # learning rate\n",
    "    eta = 0.01/n * np.eye(p)\n",
    "    # Friction rate\n",
    "    alpha = 0.1 * np.eye(p)\n",
    "\n",
    "    # Arbitrary guess at covariance of noise from mini-batching the data\n",
    "    V = np.eye(p)*1\n",
    "    niter = 500\n",
    "    batch_size=50 # make this smallish\n",
    "\n",
    "    times_all = np.zeros(7)\n",
    "    for i in range(7):\n",
    "        # run SGHMC sampler\n",
    "        t0 = time.time()\n",
    "        samps_sghmc = code.sghmc(\"mixture_of_normals\", eta, niter, alpha, theta_0, V, x, batch_size)\n",
    "        t1 = time.time()\n",
    "        times_all[i] = t1 - t0\n",
    "    print(times_all.mean(),\"s ±\",times_all.std(), \n",
    "          \"s per loop (mean ± std. dev. of 7 runs, 1 loop each)\")\n",
    "    \n",
    "    '''# ADD THIS to save plot to view\n",
    "    # plot the samples from the algorithm and save to a file\n",
    "    kdeplt = sns.kdeplot(samps_sghmc[0,:], samps_sghmc[1,:]) # Plot the joint density\n",
    "    fig = kdeplt.get_figure()\n",
    "    fig.savefig('MixNormals_example.png')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019170965467180525 s ± 0.0005318381213370563 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python test_code_mixture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.99197936, -2.98660024, -2.99559401, -3.01260938, -3.02062129,\n",
       "        -3.02501908, -3.02868635, -3.03913199, -3.06179494, -3.0805547 ,\n",
       "        -3.07576807, -3.07172615, -3.0592972 , -3.04488   , -3.04837765,\n",
       "        -3.05178238, -3.0499542 , -3.05238205, -3.05762771, -3.06240792,\n",
       "        -3.05537956, -3.05463877, -3.03902751, -3.01980831, -3.02800603,\n",
       "        -3.03859084, -3.03248457, -3.02852957, -3.0196259 , -3.01041185,\n",
       "        -2.99569372, -2.98251586, -2.96966409, -2.95634414, -2.96509888,\n",
       "        -2.97104259, -2.97095766, -2.97131935, -2.97667613, -2.98014146,\n",
       "        -2.99396871, -3.01051719, -3.01018845, -3.01394086, -3.05379943,\n",
       "        -3.09907262, -3.09587891, -3.0894229 , -3.08541307, -3.07890207,\n",
       "        -3.07805058, -3.07059222, -3.06620898, -3.06177245, -3.06829231,\n",
       "        -3.07312523, -3.07402482, -3.07094133, -3.06973363, -3.06952904,\n",
       "        -3.07653446, -3.07590842, -3.07886222, -3.09026442, -3.09666671,\n",
       "        -3.10749438, -3.10365352, -3.09913805, -3.09409876, -3.08514072,\n",
       "        -3.09913628, -3.11395934, -3.09992105, -3.08814566, -3.08012515,\n",
       "        -3.0706957 , -3.06923074, -3.06315635, -3.05489273, -3.04091937,\n",
       "        -3.03911009, -3.02289827, -3.02410787, -3.02667887, -3.01954166,\n",
       "        -3.01930193, -3.01127737, -3.006449  , -3.02070912, -3.02843652,\n",
       "        -3.02637356, -3.0266993 , -3.01661315, -2.99902494, -2.9796823 ,\n",
       "        -2.95639141, -2.93122064, -2.90407166, -2.8984483 , -2.90410645,\n",
       "        -2.92408997, -2.94612365, -2.95678866, -2.97244846, -2.97622132,\n",
       "        -2.98905663, -3.00260219, -3.00991674, -2.97820801, -2.95094015,\n",
       "        -2.96151538, -2.9751432 , -2.98349461, -2.98640779, -3.00559302,\n",
       "        -3.02260515, -3.0199649 , -3.01925562, -3.0130493 , -3.01284731,\n",
       "        -3.01089515, -3.00631765, -2.9975482 , -2.99098649, -2.9978667 ,\n",
       "        -3.00455197, -2.99817707, -2.99642567, -2.99344576, -2.9942377 ,\n",
       "        -3.00238483, -3.00936087, -2.99651915, -2.98062686, -2.97289267,\n",
       "        -2.95979898, -2.97273506, -2.98669419, -2.98315198, -2.98471853,\n",
       "        -2.98749032, -2.98851459, -2.99367429, -2.99571088, -3.00082146,\n",
       "        -3.00736447, -2.99516956, -2.98942703, -2.9863466 , -2.98308849,\n",
       "        -2.97598861, -2.96511944, -2.97163121, -2.9822342 , -2.98622355,\n",
       "        -2.99713746, -2.98305227, -2.9777895 , -2.98271782, -2.99143217,\n",
       "        -3.0015262 , -3.00802047, -2.9952162 , -2.99124766, -2.97179763,\n",
       "        -2.95355764, -2.95206453, -2.95386237, -2.96265781, -2.96952439,\n",
       "        -2.96435096, -2.96572027, -2.97265258, -2.97489546, -2.97074091,\n",
       "        -2.96778323, -2.96528848, -2.9643067 , -2.9531376 , -2.93814704,\n",
       "        -2.94013448, -2.9436961 , -2.93048905, -2.91986832, -2.92470952,\n",
       "        -2.93451986, -2.93572555, -2.94073187, -2.94714088, -2.95787872,\n",
       "        -2.96169179, -2.96409153, -2.96095644, -2.95393823, -2.96822387,\n",
       "        -2.97589495, -2.99489307, -3.01608633, -3.02545091, -3.03334293],\n",
       "       [ 3.00459871,  3.00893884,  3.00932089,  3.00776291,  3.01722304,\n",
       "         3.01441694,  2.99652198,  2.98749607,  2.99354042,  2.98868071,\n",
       "         2.98072334,  2.97568076,  2.98584666,  2.98990766,  3.00336701,\n",
       "         3.00396461,  2.99949717,  2.99543576,  2.98460499,  2.97549809,\n",
       "         2.96626565,  2.96080094,  2.96762252,  2.97342971,  2.98808395,\n",
       "         3.00136987,  3.00859147,  3.01649184,  2.99833099,  2.97436342,\n",
       "         2.97519643,  2.97523611,  2.9689348 ,  2.95796242,  2.95844468,\n",
       "         2.95835059,  2.94989196,  2.93707327,  2.91600155,  2.89564435,\n",
       "         2.90483921,  2.90600744,  2.90471727,  2.89904404,  2.92017148,\n",
       "         2.9396949 ,  2.94685417,  2.96334666,  2.95523491,  2.94707176,\n",
       "         2.94343   ,  2.94330106,  2.95040026,  2.95791993,  2.96962446,\n",
       "         2.98282844,  2.98032063,  2.97331679,  2.96810214,  2.97172248,\n",
       "         2.96972506,  2.96708231,  2.97442673,  2.97737422,  2.9603856 ,\n",
       "         2.94489565,  2.93382725,  2.9256964 ,  2.93556647,  2.94307129,\n",
       "         2.94144807,  2.94050477,  2.93231706,  2.92294716,  2.93711493,\n",
       "         2.95002495,  2.95686268,  2.96068428,  2.95231347,  2.95054156,\n",
       "         2.94391506,  2.93878902,  2.95003814,  2.95512506,  2.94633478,\n",
       "         2.93727981,  2.93684037,  2.94026315,  2.95897746,  2.96871369,\n",
       "         2.97206978,  2.98323772,  2.97958122,  2.97868593,  2.98037771,\n",
       "         2.98010915,  2.96419602,  2.94598016,  2.95003772,  2.95155165,\n",
       "         2.95335102,  2.95268327,  2.9613931 ,  2.96857813,  2.96277671,\n",
       "         2.95977792,  2.95722081,  2.95076726,  2.9481214 ,  2.9379953 ,\n",
       "         2.9446236 ,  2.94926318,  2.95770563,  2.96910917,  2.95399018,\n",
       "         2.94048569,  2.9362244 ,  2.93144365,  2.93661392,  2.93444375,\n",
       "         2.92801773,  2.92040885,  2.92927701,  2.93422026,  2.93505973,\n",
       "         2.93059021,  2.93097338,  2.92467195,  2.92499501,  2.92574476,\n",
       "         2.93731808,  2.95005384,  2.94391835,  2.93537011,  2.95367519,\n",
       "         2.96164736,  2.9795126 ,  2.99186877,  2.99549249,  2.9906591 ,\n",
       "         2.98570541,  2.99011136,  2.99060454,  2.99575465,  2.98977405,\n",
       "         2.98540823,  2.97966585,  2.97073104,  2.97081291,  2.96804717,\n",
       "         2.96045243,  2.95655895,  2.95806109,  2.95372176,  2.95348045,\n",
       "         2.95462295,  2.95447849,  2.95152825,  2.94344878,  2.93363659,\n",
       "         2.94926132,  2.96425247,  2.95339036,  2.94777345,  2.93437509,\n",
       "         2.91981386,  2.92264035,  2.92895316,  2.94138988,  2.94765387,\n",
       "         2.93729988,  2.92306701,  2.93190525,  2.94264096,  2.92938526,\n",
       "         2.9224563 ,  2.93304069,  2.93478028,  2.94858864,  2.95605208,\n",
       "         2.95173594,  2.94555232,  2.94158189,  2.93809323,  2.93958732,\n",
       "         2.93486982,  2.9560689 ,  2.97543327,  2.98536972,  2.99379163,\n",
       "         3.00168065,  3.01606375,  3.00706728,  3.00294754,  3.00941437,\n",
       "         3.01459429,  3.02332364,  3.03023678,  3.03404921,  3.04502212]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "import seaborn as sns\n",
    "\n",
    "## Example #1:\n",
    "## Sampling from a mixture of normals in 1-D\n",
    "## SAMPLING MODEL: x ~ 0.5 * N(mu1, 1) + 0.5 * N(mu2, 1)\n",
    "## PRIORS: p(mu1) = p(mu2) = N(0,10)\n",
    "\n",
    "def log_prior(theta):\n",
    "    return(-(1/(2*10))*theta.T@theta)\n",
    "      \n",
    "def log_lik(theta, x):\n",
    "    return(np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2)))\n",
    "\n",
    "def U(theta, x, n, batch_size):\n",
    "    return(-log_prior(theta) - (n/batch_size)*sum(log_lik(theta, x)))\n",
    "       \n",
    "# Automatic differentiation to get the gradient\n",
    "gradU = jacobian(U, argnum=0)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1234)\n",
    "# Set up the data\n",
    "p = 2 #dimension of theta\n",
    "theta = np.array([-3.0, 3.0]).reshape(p,-1)\n",
    "n = 100 # smaller for test\n",
    "x = np.array([np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))]).reshape(-1,1)\n",
    "\n",
    "## Initialize parameters and sample \n",
    "\n",
    "# Initialize mean parameters\n",
    "#theta_0 = np.random.normal(size=(p,1))\n",
    "theta_0 = theta # initialize at \"true\" value for testing\n",
    "\n",
    "# Initialize tuning parameters:\n",
    "# learning rate\n",
    "eta = 0.01/n * np.eye(p)\n",
    "# Friction rate\n",
    "alpha = 0.1 * np.eye(p)\n",
    "\n",
    "# Arbitrary guess at covariance of noise from mini-batching the data\n",
    "V = np.eye(p)*1\n",
    "niter = 50\n",
    "batch_size=20 # make this smallish\n",
    "\n",
    "batch_size = n\n",
    "niter = 100\n",
    "sghmc(gradU, eta, niter, alpha, theta_0, V, x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
