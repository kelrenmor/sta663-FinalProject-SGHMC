{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python Implementation of Stochastic Gradient Hamiltonian Monte Carlo with example applications from simulated and real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "#### 250 words or less. Identify 4-6 key phrases.\n",
    "\n",
    "We implement the Stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm described in the paper *Stochastic Gradient Hamiltonian Monte Carlo* by Chen et al. (2014). This particular Hamiltonian Monte Carlo (HMC) sampling method allows for a sampling 'Big Data' through the use of a noisy gradient term calculated using minibatches of the full data set. We include python implementations of the algorithm; a description of the changes made and performance gains in each successive version may be found in the **Optimization** section of this document. We include a simulated example showing the performance of the algorithm in sampling the mean parameters of a two-component mixture of normals, and a real-world example showing **FIXME**. **FIXME** this needs more stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "#### State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm. Some potential topics for the backgorund include:\n",
    "#### - What problem does it address? \n",
    "#### - What are known and possible applications of the algorithm? \n",
    "#### - What are its advantages and disadvantages relative to other algorithms?\n",
    "#### - How will you use it in your research?\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) sampling methods allow for proposing distant points in space with high acceptance probabilities. Hamiltonian Monte Carlo treats the probability density as a physical system being explored by a moving object. The object conserves energy overall, while trading between potential energy (in high probability regions) and momentum. HMC methods incorporate gradient information into the proposal distribution, which is especially advantageous for sampling a probability distribution with high correlations. This enables more efficient exploration of the space of interest than random walks, while still being in a Metropolis-Hastings framework.\n",
    "\n",
    "The paper *Stochastic Gradient Hamiltonian Monte Carlo* by Chen et al. (2014) develops a variant of HMC that sidesteps the computational limitation of HMC methods that comes from having to compute the gradient when the data are large. Specifically, the authors use a stochastic gradient and introduce a friction term that allows the algorithm to maintain the desired target distribution and invariance properties.\n",
    "\n",
    "This algorithm can be applied in any situation in which MCMC methods are needed, but is particularly useful in cases when the usual random walk methods tends to produce highly dependent samples (e.g., when there are high correlation between variables) or very low acceptance probabilities (e.g. when the sample space is very high dimensional). It also allows for scaling of Bayesian methods, with the use of minibatches of data.\n",
    "\n",
    "Other developments to improve the flexibility of HMC include the \"No U-Turn\" sampler and Riemann manifold HMC. The authors argue that their Stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm is an efficiency improvement to traditional HMC orthogonal to these other approaches, and thus could be combined with them to yield even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of algorithm\n",
    "\n",
    "First, explain in plain English what the algorithm does. Then describes the details of the algorihtm, using mathematical equations or pseudocode as appropriate. \n",
    "\n",
    "The Stochastic Gradient HMC algorithm is complex in its derivation, but simple in implementation. In words, the algorithm begins when the sampler is initialized. The algorithm draws a specified number of discrete samples by moving around the targeted probability distribution. In standard HMC each move is a deterministic function of the current location and momentum. In SGHMC, random noise is added to account for the stochastic estimation of the target distribution. In addition, during each move a friction term reduces the momentum for stability. Computationally, the algorithm is:\n",
    "\n",
    "\n",
    "*Initialize ($\\theta_0$, $v_0$, $\\alpha$, $\\eta$, $\\hat{\\beta}$)*\n",
    "\n",
    "**for t = 1,2,... :**\n",
    "    \n",
    "$\\qquad \\theta_i = \\theta_{i-1} + v$\n",
    "    \n",
    "$\\qquad v = v - \\eta \\nabla \\tilde{U(}x) - \\alpha v + N(0, 2(\\alpha - \\hat{\\beta}))$\n",
    "\n",
    "\n",
    "**end **\n",
    "\n",
    "The above computational algorithm represents the Hamiltonian dynamics, re-expressed in terms more familiar to Stochastic Gradient Descent. $\\tilde{U}(x)$ is the minibatch approximation to the gradient, $\\eta$ is the learning rate, $\\alpha$ is the friction constant, and $\\hat{\\beta}$ is the approximation to the noise introduced by the stochastic gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe optimization for performance\n",
    "\n",
    "First implement the algorithm using plain Python in a straightforward way from the description of the algorihtm. Then profile and optimize it using one or more apporpiate mathods, such as:\n",
    "\n",
    "1. Use of better algorithms or data structures\n",
    "2. Use of vectorization\n",
    "3. JIT or AOT compilation of critical functions\n",
    "4. Re-writing critical functions in C++ and using pybind11 to wrap them\n",
    "5. Making use of parallelism or concurrency\n",
    "6. Making use of distributed compuitng\n",
    "\n",
    "Document the improvemnt in performance with the optimizations performed.\n",
    "\n",
    "---\n",
    "\n",
    "Things done (see optimization_work.ipynb) include:\n",
    "1. An algorithmic performance improvement in multivariate normal sampling. We obtain improvement through the use of Cholesky decomposition based multivariate normal sampling with pre-computation of the Cholesky decomposition of covariance matrices needed.\n",
    "2. JIT compilation of our main SGHMC sampling algorithm (we tried the data batching code with and without JIT compilation for comparison within the JIT-compiled algorithm).\n",
    "\n",
    "**FIXME** describe specific gains of each step and note the example used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to simulated data sets\n",
    "\n",
    "**Are there specific inputs that give known outuputs (e.g. there might be closed form solutions for special input cases)? How does the algorithm perform on these?**\n",
    "\n",
    "**If no such input cases are available (or in addition to such input cases), how does the algorithm perform on simulated data sets for which you know the \"truth\"?**\n",
    "\n",
    "We include two simulated examples with known target distributions from which the sampler should be drawing. The first, from the Chen et al. paper, samples from the target distribution with $U(\\theta) = -2\\theta^2 + \\theta^4.$ For the SGHMC we use artificially injected gradient noise; specifically, we set $\\Delta \\tilde{U}(\\theta)=\\Delta U(\\theta) + \\mathcal{N}(0,4).$ **FIXME** give full details of initialization settings. **FIXME** show results for the SGHMC algorithm.\n",
    "\n",
    "The second example samples the mean parameters of a mixture of normals distribution. **FIXME** describe this more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to real data sets\n",
    "\n",
    "**Test the algorithm on the real-world examples in the orignal paper if possible. Try to find at least one other real-world data set not in the original paper and test it on that. Describe and interpret the results.**\n",
    "\n",
    "We still need to pick a real data set (maybe some kind of regression data set)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative analysis with competing algorihtms\n",
    "\n",
    "**Find two other algorihtms that addresss a similar problem. Perform a comparison - for example, of accurary or speed. You can use native libraires of the other algorithms - you do not need to code them yourself. Comment on your observations.**\n",
    "\n",
    "We compare the SGHMC method (as coded by us) to both the standard HMC method as implemented in the package `pyhmc`, and to **FIXME** figure out second comparison (maybe Stan?).\n",
    "\n",
    "Things to talk about: `pyhmc` is suuuuper slow at sampling when we have a gradient calculation with lots of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion/conclusion\n",
    "\n",
    "**Your thoughts on the algorithm. Does it fulfill a particular need? How could it be generalized to other problem domains? What are its limiations and how could it be improved further?**\n",
    "\n",
    "Things to talk about: the tuning parameters in the SGHMC seem very fiddly. I wouldn't trust it when it came to sampling a target distribution I knew nothing about...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References/bibliography\n",
    "\n",
    "Make sure you cite your sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The code should be in a public GitHub repository with:\n",
    "\n",
    "1. A README file\n",
    "2. An open source license\n",
    "3. Source code\n",
    "4. Test code\n",
    "5. Examples\n",
    "6. A reproducible report\n",
    "\n",
    "The package should be downloadable and installable with `python setup.py install`, or even posted to PyPI adn installable with `pip install package`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Each item is worth 10 points, but some sections will give up to 10 bonus points if done really well. Note that the \"difficulty factor\" of the chosen algorithm will be factored into the grading. \n",
    "\n",
    "1. Is the abstract, background and discussion readable and clear? (10-20 points)\n",
    "2. Is the algorithm description clear and accurate? (10-20 points)\n",
    "3. Has the algorihtm been optimized? (10-20 points)\n",
    "4. Are the applicaitons to simulated/real data clear and useful? (10-20 points)\n",
    "5. Was the comarative analysis done well? (10-20 points points)\n",
    "6. Is there a well-maitnatined Github repository for the code? (10 points)\n",
    "7. Is the document show evidenc of literate programming? (10 points)\n",
    "8. Is the analyiss reproducible? (10 points)\n",
    "9. Is the code tested? Are examples provided? (10 points)\n",
    "10. Is the package easily installable? (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
